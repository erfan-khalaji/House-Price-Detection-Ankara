{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "\n",
    "train_data_path =  \"ads_train.csv\"\n",
    "test_data_path =  \"ads_test.csv\"\n",
    "downtown_lat = 39.920966\n",
    "downtown_lng = 32.854116\n",
    "\n",
    "\n",
    "#for adjust_feature function\n",
    "train_remove_list = [\n",
    "    'quarter_name_Altınova', 'quarter_name_Beyceğiz', 'quarter_name_Dutluk',\n",
    "    'quarter_name_Erzurum', 'quarter_name_Harman', 'quarter_name_Hüseyingazi',\n",
    "    'quarter_name_Kemalpaşa', 'quarter_name_Menderes', 'quarter_name_Ostim',\n",
    "    'quarter_name_Yeşilöz', 'type_Benzin İstasyonu', 'type_Dağ Evi', 'type_Eczane',\n",
    "    'type_Köşk', 'type_Sanal & Hazır Ofis', 'type_Taksi Durağı', 'type_Villa Katı',\n",
    "    'type_Yalı Dairesi', 'type_Çiftlik Evi'\n",
    "]\n",
    "\n",
    "\n",
    "#for adjust_feature function\n",
    "test_remove_list = [\n",
    "    \n",
    "    'quarter_name_Bacı', 'quarter_name_Derbent', 'quarter_name_Kıbrısköy',\n",
    "    'quarter_name_Oyaca Yeşilçam', 'quarter_name_Subaşı', 'quarter_name_Çoğlu',\n",
    "    'type_Yazlık'\n",
    "]\n",
    "\n",
    "#Corr_lists above 0.7\n",
    "\n",
    "corr_list = [\n",
    "            'quarter_name_Çalış',\n",
    "            'quarter_name_Ortabereket',\n",
    "#             'quarter_name_Hacıkara',\n",
    "#             'lng'\n",
    "            # 'quarter_name_Hasanoğlan Bahçelievler',\n",
    "            # 'quarter_name_Saray',\n",
    "            # 'quarter_name_Saracalar',\n",
    "            # 'district_Polatlı',\n",
    "            # 'type_Daire',\n",
    "            # 'quarter_name_Merkez',\n",
    "            # 'quarter_name_Gülpınar',\n",
    "            # 'quarter_name_Fatih',\n",
    "            # 'lat',\n",
    "            # 'type_Çiftlik'\n",
    "             \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def remove_extra_features(data, train):\n",
    "    remove_list = ['city', 'currency', 'ad_title']\n",
    "    data = data.drop(remove_list, axis=1)\n",
    "    if train:\n",
    "        data = data.drop(35588)\n",
    "        data = data.reset_index(drop=True)\n",
    "        data.m2 = data.m2.astype(np.int32)\n",
    "        data = data.drop('ad_id', axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def dummy_category(data, train):\n",
    "    \n",
    "    remove_list = ['type', 'quarter_name', 'district']\n",
    "    cat_data = data.filter(['type', 'quarter_name', 'district'])\n",
    "    tempDf = pd.get_dummies(cat_data)\n",
    "    data = pd.concat([data,tempDf], axis=1)\n",
    "    \n",
    "    if train:\n",
    "        data = data.drop(remove_list, axis=1)\n",
    "    else:\n",
    "        data = data.drop(['type', 'quarter_name', 'district'], axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def downtown_distance(data):\n",
    "    \n",
    "    distance = list()\n",
    "    i=0\n",
    "    for i in range(0, len(data.lat)):\n",
    "        la = data.lat[i]\n",
    "        ln = data.lng[i]\n",
    "        location_dist = math.sqrt((la-downtown_lat)**2 + (ln-downtown_lng)**2)\n",
    "        distance.append(float(location_dist))\n",
    "    tempDf = pd.DataFrame({'distance':distance})\n",
    "    data = pd.concat([data, tempDf], axis=1)\n",
    "#     remove_list = ['lng']\n",
    "#     data = data.drop(remove_list, axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def setdiff_sorted(array1,array2,assume_unique=False):\n",
    "    ans = np.setdiff1d(array1,array2,assume_unique).tolist()\n",
    "    if assume_unique:\n",
    "        return sorted(ans)\n",
    "    return ans\n",
    "\n",
    "def adjust_features(train_data, test_data):\n",
    "    \n",
    "    cat_train_data = train_data.drop(['price','lat', 'lng', 'm2', 'posted', 'population'], axis=1)\n",
    "    cat_test_data = test_data.drop(['lat', 'lng', 'm2', 'posted', 'population'], axis=1)\n",
    "    \n",
    "    cat_train_col_list = cat_train_data.columns\n",
    "    cat_test_col_list = cat_test_data.columns\n",
    "    \n",
    "    train_data = train_data.drop(train_remove_list, axis=1)\n",
    "    test_data = test_data.drop(test_remove_list, axis=1)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def convert_posted(data):\n",
    "    data['date'] = data['posted'].apply(lambda x: datetime.datetime.strptime(str(x), '%Y%m%d').date())\n",
    "    data['year'] = data['posted'].apply(lambda x: datetime.datetime.strptime(str(x), '%Y%m%d').year)\n",
    "    data['month'] = data['posted'].apply(lambda x: datetime.datetime.strptime(str(x), '%Y%m%d').month)\n",
    "    data['day'] = data['posted'].apply(lambda x: datetime.datetime.strptime(str(x), '%Y%m%d').day)\n",
    "    data = data.drop(['date', 'posted'], axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def local_submission(clf, train_data, test_data):\n",
    "    y=train_data.price\n",
    "    X= train_data.drop('price', axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    print(\"Loss is : {}\".format(mean_squared_error(y_test, pred)))\n",
    "    \n",
    "def make_submission(clf, train_data, test_data):\n",
    "    \n",
    "    y_train = train_data.price\n",
    "    X_train = train_data.drop('price', axis=1)\n",
    "    X_test = test_data.drop('ad_id', axis=1)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    tempDf = test_data.ad_id\n",
    "    preds = pd.DataFrame({'Predicted': y_pred})\n",
    "    tempDf = pd.concat([tempDf, preds], axis=1)\n",
    "    tempDf.rename(columns = {'ad_id':'Id'}, inplace = True)\n",
    "    tempDf.Id = tempDf.astype(np.int32)\n",
    "    tempDf.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "def remove_correlated_features(train_data, test_data):\n",
    "    \n",
    "    train_data = train_data.drop(corr_list, axis=1)\n",
    "    test_data = test_data.drop(corr_list, axis=1)\n",
    "    return train_data, test_data\n",
    "def show_correlations():\n",
    "    \n",
    "    corr_matrix = train_data.corr().abs()\n",
    "    # the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\n",
    "    sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "                 .stack()\n",
    "                 .sort_values(ascending=False))\n",
    "    # first element of sol series is the pair with the bigest correlation\n",
    "    corDf = pd.DataFrame(sol)\n",
    "    return corDf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "train_data = remove_extra_features(train_data, True)\n",
    "test_data = remove_extra_features(test_data, False)\n",
    "\n",
    "train_data = dummy_category(train_data, True)\n",
    "test_data = dummy_category(test_data, False)\n",
    "\n",
    "train_data = downtown_distance(train_data)\n",
    "test_data = downtown_distance(test_data)\n",
    "\n",
    "train_data, test_data = adjust_features(train_data, test_data)\n",
    "\n",
    "train_data = convert_posted(train_data)\n",
    "test_data = convert_posted(test_data)\n",
    "\n",
    "train_data, test_data = remove_correlated_features(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:47:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:47:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:47:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:48:21] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:48:47] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:49:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:49:40] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:50:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:50:33] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:51:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "clf = BaggingRegressor(XGBRegressor())\n",
    "make_submission(clf, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
